# 本地模型配置说明

## 概述

成语猜多游戏现在支持调用本地部署的大语言模型，包括Amazon Bedrock Voice Conversation项目、Ollama、LocalAI、Text Generation WebUI、vLLM等常见的本地模型服务。

## 配置步骤

### 1. 启动本地模型服务

首先确保你的本地模型服务正在运行。常见的本地模型服务包括：

#### Amazon Bedrock Voice Conversation项目
```bash
# 确保AWS凭证已配置
export AWS_ACCESS_KEY_ID=<your-access-key>
export AWS_SECRET_ACCESS_KEY=<your-secret-key>
export AWS_DEFAULT_REGION=<your-region>
export MODEL_ID=<your-model-id>

# 项目已包含在指定目录中，无需额外启动
# 路径：D:\amazon-bedrock-voice-conversation
```

#### Ollama
```bash
# 安装Ollama后，拉取模型
ollama pull llama2

# 启动服务（默认在11434端口）
ollama serve
```

#### LocalAI
```bash
# 启动LocalAI服务
localai run --config config.yaml
```

#### Text Generation WebUI
```bash
# 启动Text Generation WebUI
python server.py --listen --port 5000
```

#### vLLM
```bash
# 启动vLLM服务
python -m vllm.entrypoints.openai.api_server --model your-model-path --port 8000
```

### 2. 在游戏中配置本地模型

1. 启动成语猜多游戏
2. 点击"设置"按钮
3. 在"语言模型选择"中选择"本地模型"
4. 根据你的模型类型填写配置：

#### 对于Amazon Bedrock Voice Conversation项目（推荐）
- **模型路径**：填写项目目录路径（如：`D:\amazon-bedrock-voice-conversation`）
- **API端点**：留空（将自动调用AWS Bedrock API）

#### 对于HTTP API模型
- **模型路径**：填写任意路径（如：`D:\amazon-bedrock-voice-conversation`）
- **API端点**：填写HTTP API地址

常见配置：
- **Amazon Bedrock**: 模型路径填写项目目录，API端点留空
- **Ollama**: `http://localhost:11434/api/generate`
- **LocalAI**: `http://localhost:8080/v1/chat/completions`
- **Text Generation WebUI**: `http://localhost:5000/api/v1/generate`
- **vLLM**: `http://localhost:8000/v1/completions`

#### 对于命令行模型
- **模型路径**：填写模型目录路径
- **API端点**：留空

### 3. 测试配置

配置完成后，点击"应用设置"按钮。如果配置正确，会显示"设置已应用"的消息。

## 常见问题

### Q: 如何知道我的模型是否支持？
A: 大多数本地模型都提供HTTP API接口。你可以查看模型的文档，或者尝试访问 `http://localhost:端口号` 来测试服务是否运行。

### Q: 模型路径和API端点有什么区别？
A: 
- **模型路径**：指向你的模型文件所在的目录，用于命令行调用
- **API端点**：HTTP API的地址，用于网络调用

如果你的模型提供HTTP API，优先使用API端点方式。

### Q: 如何测试本地模型是否正常工作？
A: 可以运行 `local_model_example.py` 文件来测试：

```bash
python local_model_example.py
```

### Q: 支持哪些本地模型？
A: 理论上支持任何提供HTTP API或命令行接口的本地模型，包括但不限于：
- Amazon Bedrock Voice Conversation项目
- Ollama (llama2, codellama, mistral等)
- LocalAI
- Text Generation WebUI
- vLLM
- LM Studio
- 自定义模型

## 高级配置

### 自定义模型接口

如果你的模型有特殊的接口格式，可以修改 `local_model_example.py` 文件中的函数来适配：

```python
def _call_http_api(prompt: str, max_tokens: int, temperature: float) -> str:
    # 修改这里的代码来适配你的模型API
    api_url = "http://your-model-api-endpoint"
    
    data = {
        "prompt": prompt,
        "max_tokens": max_tokens,
        "temperature": temperature
    }
    
    response = requests.post(api_url, json=data, timeout=30)
    # 根据你的模型响应格式修改解析逻辑
    return response.json()["response"]
```

### 环境变量配置

你也可以通过环境变量来配置模型：

```bash
export LOCAL_MODEL_PATH="D:\amazon-bedrock-voice-conversation"
export LOCAL_MODEL_API="http://localhost:8000/generate"
```

## 故障排除

### 1. 连接超时
- 检查模型服务是否正在运行
- 确认端口号是否正确
- 检查防火墙设置

### 2. API格式错误
- 查看模型服务的API文档
- 修改 `local_model_example.py` 中的请求格式
- 检查响应解析逻辑

### 3. 模型路径错误
- 确认路径存在且有读取权限
- 检查路径中是否包含特殊字符
- 使用绝对路径而不是相对路径

### 4. 权限问题
- 确保Python有权限访问模型目录
- 检查文件系统权限
- 在Windows上可能需要以管理员身份运行

## 性能优化

### 1. 减少延迟
- 使用本地模型可以减少网络延迟
- 调整模型的batch size和并发设置
- 使用更快的硬件（GPU加速）

### 2. 内存优化
- 选择合适的模型大小
- 使用量化模型（如GGUF格式）
- 调整max_tokens参数

### 3. 并发处理
- 如果支持，启用模型的并发处理
- 调整timeout参数
- 使用异步调用（如果模型支持）

## 示例配置

### Amazon Bedrock Voice Conversation配置
```python
# 模型路径：D:\amazon-bedrock-voice-conversation
# API端点：留空（自动使用AWS Bedrock）
# 环境变量：
# AWS_ACCESS_KEY_ID=<your-access-key>
# AWS_SECRET_ACCESS_KEY=<your-secret-key>
# AWS_DEFAULT_REGION=<your-region>
# MODEL_ID=<your-model-id>
```

### Ollama配置
```python
# 模型路径：任意路径
# API端点：http://localhost:11434/api/generate
```

### LocalAI配置
```python
# 模型路径：任意路径  
# API端点：http://localhost:8080/v1/chat/completions
```

### Text Generation WebUI配置
```python
# 模型路径：任意路径
# API端点：http://localhost:5000/api/v1/generate
```

### vLLM配置
```python
# 模型路径：任意路径
# API端点：http://localhost:8000/v1/completions
```

## 联系支持

如果遇到问题，可以：
1. 查看游戏日志中的错误信息
2. 测试模型API是否正常工作
3. 参考 `local_model_example.py` 中的示例代码
4. 检查模型服务的官方文档 